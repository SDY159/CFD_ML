import sys
import os
import pandas as pd
import numpy as np
import glob
import csv
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score
from numpy import array
import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM, RepeatVector, Reshape, TimeDistributed, InputLayer, Conv3D, MaxPooling3D
from tensorflow.keras.utils import to_categorical

from tensorflow.keras.utils import to_categorical
from sklearn import preprocessing

#Print all of content
np.set_printoptions(threshold=5000, edgeitems=20, linewidth=100)

#Use delimeter as comma
#np.set_string_function(lambda x: repr(x), repr=False)

# fix random seed for reproducibility
seed = 7
np.random.seed(7)


#check list as np array
#np.array(arr_final).shape


#load files

# read x
path = ('/bigdata/wonglab/syang159/particle_size3/')
li = []

# reading files

for i in np.arange(4000, 4101, 2):
    os.chdir(path + "cell_0.%08i" % i + '/time/')
    all_files = glob.glob(path + "cell_0.%08i" % i + '/time/' + "/*.csv")
    all_files = sorted(os.listdir(path + "cell_0.%08i" % i + '/time/'), key=lambda x: int(x.replace(".csv", "")))
    for filename in all_files:
        df = pd.read_csv(filename, index_col=0, header=0)
        li.append(df)
frame = pd.concat(li)


#getting data to make (180x10x6) arrays
grpdat = frame.groupby('particle')

#delete 1st and 8th column
del frame['particle']
del frame['time']

#indepent input normalization
normalized_pos_x = np.array(frame['pos_x'])
normalized_pos_x = preprocessing.normalize([normalized_pos_x], norm='max')
normalized_pos_x= np.squeeze(normalized_pos_x)
frame['pos_x'] =normalized_pos_x
frame['pos_x'] = pd.to_numeric(frame['pos_x'])

normalized_pos_y = np.array(frame['pos_y'])
normalized_pos_y = preprocessing.normalize([normalized_pos_y], norm='max')
normalized_pos_y= np.squeeze(normalized_pos_y)
frame['pos_y'] =normalized_pos_y
frame['pos_y'] = pd.to_numeric(frame['pos_y'])

normalized_pos_z = np.array(frame['pos_z'])
normalized_pos_z = preprocessing.normalize([normalized_pos_z], norm='max')
normalized_pos_z= np.squeeze(normalized_pos_z)
frame['pos_z'] =normalized_pos_z
frame['pos_z'] = pd.to_numeric(frame['pos_z'])

normalized_vel_x = np.array(frame['vel_x'])
normalized_vel_x = preprocessing.normalize([normalized_vel_x], norm='max')
normalized_vel_x= np.squeeze(normalized_vel_x)
frame['vel_x'] =normalized_vel_x
frame['vel_x'] = pd.to_numeric(frame['vel_x'])

normalized_vel_y = np.array(frame['vel_y'])
normalized_vel_y = preprocessing.normalize([normalized_vel_y], norm='max')
normalized_vel_y= np.squeeze(normalized_vel_y)
frame['vel_y'] =normalized_vel_y
frame['vel_y'] = pd.to_numeric(frame['vel_y'])

normalized_vel_z = np.array(frame['vel_z'])
normalized_vel_z = preprocessing.normalize([normalized_vel_z], norm='max')
normalized_vel_z= np.squeeze(normalized_vel_z)
frame['vel_z'] =normalized_vel_z
frame['vel_z'] = pd.to_numeric(frame['vel_z'])

#adding particle size info
frame.insert(6, "particle_size", 0.0)
frame['particle_size'] = pd.to_numeric(frame['particle_size'])
frame = frame.reset_index()
frame = frame.reset_index()
frame['particle_size'] = frame['level_0']/9000*0.00000002 + 0.00004000
del frame['level_0']

#make 4D input array with normalized particle size
normalized_particle_size = np.array(frame['particle_size'])
normalized_particle_size = preprocessing.normalize([normalized_particle_size], norm='max')
normalized_particle_size= np.squeeze(normalized_particle_size)
frame['particle_size'] =normalized_particle_size
frame['particle_size'] = pd.to_numeric(frame['particle_size'])

arr_final = []


for i in range (1, 181, 1):

        arr_i = grpdat.get_group(i).values
        arr_split_by_time = np.array_split(arr_i, 51)
        arr_final.append(arr_split_by_time)

arr_final = np.transpose(arr_final, (2, 1, 0, 3))


X1 = arr_final[0]

X1 = np.reshape(X1, (51, 180*7))

X2 = np.transpose(arr_final, (1, 0, 2, 3))

# read y
liy=[]


for i in np.arange(4000, 4101, 2):
    df_1  = pd.read_csv(path + "cell_0.%08i" % i + "/corrosion_pressure")
    lst_1 = range(0,414)
    df_1 = df_1.drop(lst_1)
    lst_2 = range (8426,8840)
    df_1 = df_1.drop(lst_2)
    erosion = np.array(df_1["dpm-erosion-rate-finnie"])
    liy.append(erosion)

y = np.array(liy)
y = preprocessing.normalize(y, norm='max')



#declare k fold splits
cvscores=[]
predicted_y=[]
kfold = KFold(n_splits=5, shuffle=True, random_state=seed)

#RNN layer
for train, test in kfold.split(X1,X2):

    model1 = Sequential()
    model1.add(Dense(1260, input_shape=(1260,)))
    model1.add(RepeatVector(50))
    model1.add(LSTM(1260, return_sequences=True))
    model1.add(LSTM(1260, return_sequences=True))
    model1.add(LSTM(1260, return_sequences=True))
    model1.add(TimeDistributed(Dense(180*7)))
    model1.add(TimeDistributed(Reshape((180, 7))))
    model1.add(Reshape(target_shape=(50, 180, 7, 1)))


#CNN layer
for train, test in kfold.split(X2, y):

    model2 = Sequential()
    model2.add(Conv3D(30, kernel_size=(3, 3, 3), activation='tanh', padding='same', data_format='channels_last', input_shape= (50, 180, 7, 1)))
    model2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
    model2.add(Conv3D(60, kernel_size=(2, 2, 2), activation='tanh',padding='same',  data_format='channels_last'))
    model2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
    model2.add(Conv3D(90, kernel_size=(2, 2, 2), activation='tanh',padding='same',  data_format='channels_last'))
    model2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
    model2.add(Conv3D(120, kernel_size=(2, 2, 2), activation='tanh',padding='same',  data_format='channels_last'))
    model2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
    model2.add(Conv3D(150, kernel_size=(2, 2, 2), activation='tanh',padding='same',  data_format='channels_last'))
    model2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
    model2.add(Conv3D(180, kernel_size=(2, 2, 2), activation='tanh',padding='same',  data_format='channels_last'))
    model2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
    model2.add(Conv3D(210, kernel_size=(2, 2, 2), activation='tanh',padding='same',  data_format='channels_last'))
    model2.add(MaxPooling3D(pool_size=(2, 2, 2), padding='same'))
    model2.add(Flatten())
    #model2.add(Dense(8012, activation='relu'))

    merged=Concatenate()([model1.output, model2.output])
   
    # Compile the model
    model.compile(optimizer='adam', loss='mse')
    model.fit(X[train], y[train], epochs=20, batch_size=1001, verbose=0)
    #print(model.predict(X[test]))
    #print("==================================================")
    #print(y[test])


    predicted_y = np.array(model.predict(X[test]))
    flat_pre_y = predicted_y.flatten()
    flat_ori_y = y[test].flatten()
    os.chdir(path)


    # predicted results and test set into csv
    a = np.asarray(flat_pre_y)
    np.savetxt('predicted.csv', a, fmt='%.8e', delimiter=",")
    b = np.asarray(flat_ori_y)
    np.savetxt('original.csv', b, fmt='%.8e', delimiter=",")

	# evaluate the model
    scores = r2_score(flat_ori_y, flat_pre_y)

    print("r2_score: %.5f" % r2_score(flat_ori_y, flat_pre_y))
    cvscores.append(scores)
		
print("Average_r2_score: %.5f (+/- %.5f)" % (np.mean(cvscores), np.std(cvscores)))
